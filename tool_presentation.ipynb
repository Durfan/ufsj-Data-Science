{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Downloading numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 916 kB/s \n",
      "\u001b[?25hCollecting pytz>=2017.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./.venv/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.21.4 pandas-1.3.4 pytz-2021.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/cecilio/Projetos/ufsj-Data-Science/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (1.21.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/cecilio/Projetos/ufsj-Data-Science/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting faiss-cpu\n",
      "  Using cached faiss_cpu-1.7.1.post2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.4 MB)\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.1.post2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/cecilio/Projetos/ufsj-Data-Science/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 4.0 MB/s \n",
      "\u001b[?25hCollecting tokenizers>=0.10.3\n",
      "  Using cached tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.10.0-cp39-cp39-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 881.9 MB 490 bytes/s \n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.11.1-cp39-cp39-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 11.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.venv/lib/python3.9/site-packages (from sentence-transformers) (1.21.4)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 39.8 MB 128 kB/s \n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 9.2 MB/s \n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 1.8 MB/s \n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.3.2-py3-none-any.whl (9.7 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-21.2-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 2.6 MB/s \n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2021.11.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (763 kB)\n",
      "\u001b[K     |████████████████████████████████| 763 kB 11.3 MB/s \n",
      "\u001b[?25hCollecting requests\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[K     |████████████████████████████████| 661 kB 10.3 MB/s \n",
      "\u001b[?25hCollecting pyparsing<3,>=2.0.2\n",
      "  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 3.6 MB/s \n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "\u001b[K     |████████████████████████████████| 149 kB 10.3 MB/s \n",
      "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.9/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Collecting pillow!=8.3.0,>=5.3.0\n",
      "  Using cached Pillow-8.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using legacy 'setup.py install' for sentence-transformers, since package 'wheel' is not installed.\n",
      "Installing collected packages: urllib3, pyparsing, idna, charset-normalizer, certifi, typing-extensions, tqdm, requests, regex, pyyaml, packaging, joblib, filelock, click, torch, tokenizers, threadpoolctl, scipy, sacremoses, pillow, huggingface-hub, transformers, torchvision, sentencepiece, scikit-learn, nltk, sentence-transformers\n",
      "    Running setup.py install for sentence-transformers ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed certifi-2021.10.8 charset-normalizer-2.0.7 click-8.0.3 filelock-3.3.2 huggingface-hub-0.1.2 idna-3.3 joblib-1.1.0 nltk-3.6.5 packaging-21.2 pillow-8.4.0 pyparsing-2.4.7 pyyaml-6.0 regex-2021.11.10 requests-2.26.0 sacremoses-0.0.46 scikit-learn-1.0.1 scipy-1.7.2 sentence-transformers-2.1.0 sentencepiece-0.1.96 threadpoolctl-3.0.0 tokenizers-0.10.3 torch-1.10.0 torchvision-0.11.1 tqdm-4.62.3 transformers-4.12.3 typing-extensions-3.10.0.2 urllib3-1.26.7\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/cecilio/Projetos/ufsj-Data-Science/.venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install faiss-cpu\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings/Models explicação\n",
    "\n",
    "- [https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526](https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526)\n",
    "- [https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html](https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html)\n",
    "- [https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html)\n",
    "- [https://github.com/facebookresearch/faiss/wiki/Faiss-indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "data_path = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentenceTransformers\n",
    "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\n",
    "\n",
    "# Faiss\n",
    "Faiss is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_index(data):\n",
    "    encoded_data = model.encode(data)\n",
    "    encoded_data = np.asarray(encoded_data.astype('float32'))\n",
    "    index = faiss.IndexIDMap(faiss.IndexFlatIP(768))\n",
    "    ids = np.array(range(0, len(data)))\n",
    "    ids = np.asarray(ids.astype('int64'))\n",
    "    index.add_with_ids(encoded_data, ids)\n",
    "    faiss.write_index(index, f'{data_path}/sentences.index')\n",
    "\n",
    "def search(query):\n",
    "    index = faiss.read_index(f'{data_path}/sentences.index')\n",
    "    query_vector = model.encode([query])\n",
    "    k = 5\n",
    "    top_k = index.search(query_vector, k)\n",
    "    return top_k[1].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{data_path}/data_file.csv', sep=\"\\t\")\n",
    "data = df[\"col\"]\n",
    "\n",
    "generate_index(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"string\"\n",
    "\n",
    "search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussão dos Resultados\n",
    "\n",
    "- Comparação de metodos e modelos\n",
    "- Criar gráficos"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6331c55575620c9120a3df92456d8d2d3a37aa30021015ffc2536ee6df0b250b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
